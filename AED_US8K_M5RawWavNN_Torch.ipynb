{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#import torchaudio\n",
    "import librosa #use librosa instead of torchaudio since we're on Win...\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#check if Cuda is avaliable\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset\n",
    "data_csv = pd.read_csv(os.getcwd()+\"\\\\dataset\\\\UrbanSound8K\\\\metadata\\\\UrbanSound8K.csv\") \n",
    "data_csv.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting dataset\n",
    "class US8K(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_path, file_path, folder_list):\n",
    "        \n",
    "        self.files_name = []\n",
    "        self.labels = []\n",
    "        self.folders = []\n",
    "        \n",
    "        #read metadata from .csv\n",
    "        csv_metadata = pd.read_csv(csv_path)\n",
    "        \n",
    "        #loop through the metadata and save them into the lists\n",
    "        for i in range(0, len(csv_metadata)):\n",
    "            if csv_metadata.iloc[i, 5] in folder_list:\n",
    "                self.files_name.append(csv_metadata.iloc[i,0])\n",
    "                self.labels.append(csv_metadata.iloc[i,6])\n",
    "                self.folders.append(csv_metadata.iloc[i,5])\n",
    "            \n",
    "        self.filePath = file_path\n",
    "        self.folderList = folder_list\n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #format audio file path\n",
    "        path = self.filePath + \"fold\" + str(self.folders[index]) + \"\\\\\" + self.files_name[index]\n",
    "        #load audio data(sampling rate = 22.5K; downmixed to mono)\n",
    "        audio_data = librosa.core.load(path, sr=8000, mono=True)[0]\n",
    "        #reshape audio data [1,n_frames]\n",
    "        audio_data = np.reshape(audio_data,(audio_data.size,1))\n",
    "        #convert np array to tensor\n",
    "        audio_data = torch.from_numpy(audio_data)\n",
    "        \n",
    "         #downsample the audio to ~8kHz\n",
    "        tempData = torch.zeros([160000, 1]) #tempData accounts for audio clips that are too short\n",
    "        if audio_data.numel() < 160000:\n",
    "            \n",
    "            tempData[:audio_data.numel()] = audio_data[:]\n",
    "        else:\n",
    "            tempData[:] = audio_data[:160000]\n",
    "\n",
    "        audio_data = tempData\n",
    "        soundFormatted = torch.zeros([32000, 1])\n",
    "        soundFormatted[:32000] = audio_data[::5] #take every fifth sample of soundData\n",
    "        soundFormatted = soundFormatted.permute(1, 0)\n",
    "        \n",
    "\n",
    "        \n",
    "        return soundFormatted, self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 6273 files\n",
      "Validation set size: 1622 files\n",
      "Test set size: 837 files\n"
     ]
    }
   ],
   "source": [
    "csv_path = os.getcwd() + '\\\\dataset\\\\UrbanSound8K\\\\metadata\\\\UrbanSound8K.csv'\n",
    "file_path = os.getcwd() + '\\\\dataset\\\\UrbanSound8K\\\\audio\\\\'\n",
    "\n",
    "train_set = US8K(csv_path, file_path, range(1,8))\n",
    "validation_set = US8K(csv_path, file_path, range(8,10))\n",
    "test_set = US8K(csv_path, file_path, [10])\n",
    "\n",
    "print(\"Train set size: \" + str(len(train_set)) + ' files')\n",
    "print(\"Validation set size: \" + str(len(validation_set)) + ' files')\n",
    "print(\"Test set size: \" + str(len(test_set)) + ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True, **kwargs)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size = 128, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(1, 128, kernel_size=(80,), stride=(4,))\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgPool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
      "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    #M5 CNN: m5 denotes 5 weighted layers\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        #first layer\n",
    "        #a convolutional layer with receptive field 882 and 128 filters, with stride 4 \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, # input height\n",
    "                      out_channels=128, # n_filters\n",
    "                      kernel_size= 80, # filter size; 40ms=> kernel size = 0.04 * sampling rate = 882\n",
    "                      stride = 4), #hop size\n",
    "            nn.BatchNorm1d(128), # batch normalization\n",
    "            nn.ReLU(), #activation func\n",
    "            nn.MaxPool1d(4), # max pooling\n",
    "        )  \n",
    "        \n",
    "        #second layer\n",
    "        #a convolutional layer with receptive field 3 and 128 filters\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, # input height\n",
    "                      out_channels=128, # n_filters\n",
    "                      kernel_size= 3), # filter size\n",
    "            nn.BatchNorm1d(128), # batch normalization\n",
    "            nn.ReLU(), #activation func\n",
    "            nn.MaxPool1d(4), # max pooling\n",
    "        )\n",
    "        \n",
    "        #third layer\n",
    "        #a convolutional layer with receptive field 3 and 256 filters\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, # input height\n",
    "                      out_channels=256, # n_filters\n",
    "                      kernel_size= 3), # filter size\n",
    "            nn.BatchNorm1d(256), # batch normalization\n",
    "            nn.ReLU(), #activation func\n",
    "            nn.MaxPool1d(4), # max pooling\n",
    "        )\n",
    "        \n",
    "        #fourth layer\n",
    "        #a convolutional layer with receptive field 3 and 512 filters\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, # input height\n",
    "                      out_channels=512, # n_filters\n",
    "                      kernel_size= 3), # filter size\n",
    "            nn.BatchNorm1d(512), # batch normalization\n",
    "            nn.ReLU(), #activation func\n",
    "            nn.MaxPool1d(4), # max pooling\n",
    "        )\n",
    "        \n",
    "        self.avgPool = nn.AvgPool1d(30)\n",
    "        self.fc1 = nn.Linear(512, 10)#fully connected layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = self.avgPool(x)\n",
    "        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim = 2)\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "print(model)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #clear gradients for this training step\n",
    "        optimizer.zero_grad()\n",
    "        #move to gpu if available\n",
    "        data = data.to(device)\n",
    "        #target is the ground truth label\n",
    "        target = target.to(device)\n",
    "        #set requires_grad to True for training\n",
    "        data = data.requires_grad_() \n",
    "        #computation\n",
    "        output = model(data)\n",
    "        #original output dimensions are batchSizex1x10\n",
    "        output = output.permute(1, 0, 2) \n",
    "        #the loss functions expects a batchSizex10 input\n",
    "        loss = F.nll_loss(output[0], target) \n",
    "        #backpropagation, compute gradients\n",
    "        loss.backward()\n",
    "        # apply gradients\n",
    "        optimizer.step()\n",
    "        #print training stats\n",
    "        if batch_idx % log_interval == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))\n",
    "            \n",
    "def validation(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(2)[1]\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "    print('\\Validation set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(validation_loader.dataset),\n",
    "        100. * correct / len(validation_loader.dataset)))\n",
    "    \n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        pred = output.max(2)[1] \n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6273 (0%)]\tLoss: 2.305035\n",
      "Train Epoch: 1 [2560/6273 (40%)]\tLoss: 1.812353\n",
      "Train Epoch: 1 [5120/6273 (80%)]\tLoss: 1.773938\n",
      "\n",
      "Test set: Accuracy: 159/837 (19%)\n",
      "\n",
      "Train Epoch: 2 [0/6273 (0%)]\tLoss: 1.508778\n",
      "Train Epoch: 2 [2560/6273 (40%)]\tLoss: 1.679536\n",
      "Train Epoch: 2 [5120/6273 (80%)]\tLoss: 1.303563\n",
      "\n",
      "Test set: Accuracy: 143/837 (17%)\n",
      "\n",
      "Train Epoch: 3 [0/6273 (0%)]\tLoss: 1.661988\n",
      "Train Epoch: 3 [2560/6273 (40%)]\tLoss: 1.532936\n",
      "Train Epoch: 3 [5120/6273 (80%)]\tLoss: 1.580673\n",
      "\n",
      "Test set: Accuracy: 140/837 (17%)\n",
      "\n",
      "Train Epoch: 4 [0/6273 (0%)]\tLoss: 1.587494\n",
      "Train Epoch: 4 [2560/6273 (40%)]\tLoss: 1.414286\n",
      "Train Epoch: 4 [5120/6273 (80%)]\tLoss: 1.289022\n",
      "\n",
      "Test set: Accuracy: 192/837 (23%)\n",
      "\n",
      "Train Epoch: 5 [0/6273 (0%)]\tLoss: 1.430935\n",
      "Train Epoch: 5 [2560/6273 (40%)]\tLoss: 1.481584\n",
      "Train Epoch: 5 [5120/6273 (80%)]\tLoss: 1.238414\n",
      "\n",
      "Test set: Accuracy: 244/837 (29%)\n",
      "\n",
      "Train Epoch: 6 [0/6273 (0%)]\tLoss: 1.308653\n",
      "Train Epoch: 6 [2560/6273 (40%)]\tLoss: 1.600027\n",
      "Train Epoch: 6 [5120/6273 (80%)]\tLoss: 1.106309\n",
      "\n",
      "Test set: Accuracy: 292/837 (35%)\n",
      "\n",
      "Train Epoch: 7 [0/6273 (0%)]\tLoss: 1.248514\n",
      "Train Epoch: 7 [2560/6273 (40%)]\tLoss: 1.337111\n",
      "Train Epoch: 7 [5120/6273 (80%)]\tLoss: 1.197403\n",
      "\n",
      "Test set: Accuracy: 173/837 (21%)\n",
      "\n",
      "Train Epoch: 8 [0/6273 (0%)]\tLoss: 1.524010\n",
      "Train Epoch: 8 [2560/6273 (40%)]\tLoss: 1.326057\n",
      "Train Epoch: 8 [5120/6273 (80%)]\tLoss: 1.377108\n",
      "\n",
      "Test set: Accuracy: 300/837 (36%)\n",
      "\n",
      "Train Epoch: 9 [0/6273 (0%)]\tLoss: 1.417879\n",
      "Train Epoch: 9 [2560/6273 (40%)]\tLoss: 1.255914\n",
      "Train Epoch: 9 [5120/6273 (80%)]\tLoss: 1.401481\n",
      "\n",
      "Test set: Accuracy: 317/837 (38%)\n",
      "\n",
      "Train Epoch: 10 [0/6273 (0%)]\tLoss: 1.438321\n",
      "Train Epoch: 10 [2560/6273 (40%)]\tLoss: 1.246739\n",
      "Train Epoch: 10 [5120/6273 (80%)]\tLoss: 1.090226\n",
      "\n",
      "Test set: Accuracy: 213/837 (25%)\n",
      "\n",
      "Train Epoch: 11 [0/6273 (0%)]\tLoss: 1.211241\n",
      "Train Epoch: 11 [2560/6273 (40%)]\tLoss: 1.116657\n",
      "Train Epoch: 11 [5120/6273 (80%)]\tLoss: 1.227180\n",
      "\n",
      "Test set: Accuracy: 331/837 (40%)\n",
      "\n",
      "Train Epoch: 12 [0/6273 (0%)]\tLoss: 1.430570\n",
      "Train Epoch: 12 [2560/6273 (40%)]\tLoss: 1.451170\n",
      "Train Epoch: 12 [5120/6273 (80%)]\tLoss: 1.173923\n",
      "\n",
      "Test set: Accuracy: 269/837 (32%)\n",
      "\n",
      "Train Epoch: 13 [0/6273 (0%)]\tLoss: 1.269671\n",
      "Train Epoch: 13 [2560/6273 (40%)]\tLoss: 1.161870\n",
      "Train Epoch: 13 [5120/6273 (80%)]\tLoss: 1.221771\n",
      "\n",
      "Test set: Accuracy: 294/837 (35%)\n",
      "\n",
      "Train Epoch: 14 [0/6273 (0%)]\tLoss: 1.178194\n",
      "Train Epoch: 14 [2560/6273 (40%)]\tLoss: 1.186597\n",
      "Train Epoch: 14 [5120/6273 (80%)]\tLoss: 1.122308\n",
      "\n",
      "Test set: Accuracy: 360/837 (43%)\n",
      "\n",
      "Train Epoch: 15 [0/6273 (0%)]\tLoss: 1.367856\n",
      "Train Epoch: 15 [2560/6273 (40%)]\tLoss: 1.048852\n",
      "Train Epoch: 15 [5120/6273 (80%)]\tLoss: 1.194851\n",
      "\n",
      "Test set: Accuracy: 256/837 (31%)\n",
      "\n",
      "Train Epoch: 16 [0/6273 (0%)]\tLoss: 1.466359\n",
      "Train Epoch: 16 [2560/6273 (40%)]\tLoss: 1.054223\n",
      "Train Epoch: 16 [5120/6273 (80%)]\tLoss: 1.107624\n",
      "\n",
      "Test set: Accuracy: 333/837 (40%)\n",
      "\n",
      "Train Epoch: 17 [0/6273 (0%)]\tLoss: 1.222169\n",
      "Train Epoch: 17 [2560/6273 (40%)]\tLoss: 1.275859\n",
      "Train Epoch: 17 [5120/6273 (80%)]\tLoss: 1.258144\n",
      "\n",
      "Test set: Accuracy: 298/837 (36%)\n",
      "\n",
      "Train Epoch: 18 [0/6273 (0%)]\tLoss: 1.569299\n",
      "Train Epoch: 18 [2560/6273 (40%)]\tLoss: 1.240244\n",
      "Train Epoch: 18 [5120/6273 (80%)]\tLoss: 1.290512\n",
      "\n",
      "Test set: Accuracy: 370/837 (44%)\n",
      "\n",
      "Train Epoch: 19 [0/6273 (0%)]\tLoss: 1.323132\n",
      "Train Epoch: 19 [2560/6273 (40%)]\tLoss: 1.296067\n",
      "Train Epoch: 19 [5120/6273 (80%)]\tLoss: 1.159137\n",
      "\n",
      "Test set: Accuracy: 262/837 (31%)\n",
      "\n",
      "Train Epoch: 20 [0/6273 (0%)]\tLoss: 0.971709\n",
      "Train Epoch: 20 [2560/6273 (40%)]\tLoss: 1.082089\n",
      "Train Epoch: 20 [5120/6273 (80%)]\tLoss: 0.963208\n",
      "\n",
      "Test set: Accuracy: 428/837 (51%)\n",
      "\n",
      "Train Epoch: 21 [0/6273 (0%)]\tLoss: 0.905349\n",
      "Train Epoch: 21 [2560/6273 (40%)]\tLoss: 0.935418\n",
      "Train Epoch: 21 [5120/6273 (80%)]\tLoss: 0.963853\n",
      "\n",
      "Test set: Accuracy: 449/837 (54%)\n",
      "\n",
      "Train Epoch: 22 [0/6273 (0%)]\tLoss: 0.981544\n",
      "Train Epoch: 22 [2560/6273 (40%)]\tLoss: 0.868107\n",
      "Train Epoch: 22 [5120/6273 (80%)]\tLoss: 0.755420\n",
      "\n",
      "Test set: Accuracy: 449/837 (54%)\n",
      "\n",
      "Train Epoch: 23 [0/6273 (0%)]\tLoss: 0.781902\n",
      "Train Epoch: 23 [2560/6273 (40%)]\tLoss: 0.834897\n",
      "Train Epoch: 23 [5120/6273 (80%)]\tLoss: 0.997499\n",
      "\n",
      "Test set: Accuracy: 462/837 (55%)\n",
      "\n",
      "Train Epoch: 24 [0/6273 (0%)]\tLoss: 0.948198\n",
      "Train Epoch: 24 [2560/6273 (40%)]\tLoss: 0.925442\n",
      "Train Epoch: 24 [5120/6273 (80%)]\tLoss: 0.886580\n",
      "\n",
      "Test set: Accuracy: 471/837 (56%)\n",
      "\n",
      "Train Epoch: 25 [0/6273 (0%)]\tLoss: 0.994591\n",
      "Train Epoch: 25 [2560/6273 (40%)]\tLoss: 0.983053\n",
      "Train Epoch: 25 [5120/6273 (80%)]\tLoss: 0.791315\n",
      "\n",
      "Test set: Accuracy: 458/837 (55%)\n",
      "\n",
      "Train Epoch: 26 [0/6273 (0%)]\tLoss: 0.849755\n",
      "Train Epoch: 26 [2560/6273 (40%)]\tLoss: 1.046988\n",
      "Train Epoch: 26 [5120/6273 (80%)]\tLoss: 0.799644\n",
      "\n",
      "Test set: Accuracy: 466/837 (56%)\n",
      "\n",
      "Train Epoch: 27 [0/6273 (0%)]\tLoss: 0.849504\n",
      "Train Epoch: 27 [2560/6273 (40%)]\tLoss: 0.928560\n",
      "Train Epoch: 27 [5120/6273 (80%)]\tLoss: 0.878066\n",
      "\n",
      "Test set: Accuracy: 457/837 (55%)\n",
      "\n",
      "Train Epoch: 28 [0/6273 (0%)]\tLoss: 0.933635\n",
      "Train Epoch: 28 [2560/6273 (40%)]\tLoss: 0.759602\n",
      "Train Epoch: 28 [5120/6273 (80%)]\tLoss: 0.820332\n",
      "\n",
      "Test set: Accuracy: 441/837 (53%)\n",
      "\n",
      "Train Epoch: 29 [0/6273 (0%)]\tLoss: 0.756746\n",
      "Train Epoch: 29 [2560/6273 (40%)]\tLoss: 0.762559\n",
      "Train Epoch: 29 [5120/6273 (80%)]\tLoss: 0.788942\n",
      "\n",
      "Test set: Accuracy: 458/837 (55%)\n",
      "\n",
      "Train Epoch: 30 [0/6273 (0%)]\tLoss: 0.823820\n",
      "Train Epoch: 30 [2560/6273 (40%)]\tLoss: 0.906525\n",
      "Train Epoch: 30 [5120/6273 (80%)]\tLoss: 0.822244\n",
      "\n",
      "Test set: Accuracy: 463/837 (55%)\n",
      "\n",
      "First round of training complete. Setting learn rate to 0.001.\n",
      "Train Epoch: 31 [0/6273 (0%)]\tLoss: 0.707088\n",
      "Train Epoch: 31 [2560/6273 (40%)]\tLoss: 0.762730\n",
      "Train Epoch: 31 [5120/6273 (80%)]\tLoss: 0.843157\n",
      "\n",
      "Test set: Accuracy: 435/837 (52%)\n",
      "\n",
      "Train Epoch: 32 [0/6273 (0%)]\tLoss: 0.847592\n",
      "Train Epoch: 32 [2560/6273 (40%)]\tLoss: 0.661535\n",
      "Train Epoch: 32 [5120/6273 (80%)]\tLoss: 0.735652\n",
      "\n",
      "Test set: Accuracy: 421/837 (50%)\n",
      "\n",
      "Train Epoch: 33 [0/6273 (0%)]\tLoss: 0.871145\n",
      "Train Epoch: 33 [2560/6273 (40%)]\tLoss: 0.693515\n",
      "Train Epoch: 33 [5120/6273 (80%)]\tLoss: 0.743099\n",
      "\n",
      "Test set: Accuracy: 464/837 (55%)\n",
      "\n",
      "Train Epoch: 34 [0/6273 (0%)]\tLoss: 0.609815\n",
      "Train Epoch: 34 [2560/6273 (40%)]\tLoss: 0.800080\n",
      "Train Epoch: 34 [5120/6273 (80%)]\tLoss: 0.836383\n",
      "\n",
      "Test set: Accuracy: 453/837 (54%)\n",
      "\n",
      "Train Epoch: 35 [0/6273 (0%)]\tLoss: 0.687876\n",
      "Train Epoch: 35 [2560/6273 (40%)]\tLoss: 0.640250\n",
      "Train Epoch: 35 [5120/6273 (80%)]\tLoss: 0.741923\n",
      "\n",
      "Test set: Accuracy: 451/837 (54%)\n",
      "\n",
      "Train Epoch: 36 [0/6273 (0%)]\tLoss: 0.762574\n",
      "Train Epoch: 36 [2560/6273 (40%)]\tLoss: 0.827541\n",
      "Train Epoch: 36 [5120/6273 (80%)]\tLoss: 0.742929\n",
      "\n",
      "Test set: Accuracy: 468/837 (56%)\n",
      "\n",
      "Train Epoch: 37 [0/6273 (0%)]\tLoss: 0.739265\n",
      "Train Epoch: 37 [2560/6273 (40%)]\tLoss: 0.783539\n",
      "Train Epoch: 37 [5120/6273 (80%)]\tLoss: 0.781126\n",
      "\n",
      "Test set: Accuracy: 448/837 (54%)\n",
      "\n",
      "Train Epoch: 38 [0/6273 (0%)]\tLoss: 0.725049\n",
      "Train Epoch: 38 [2560/6273 (40%)]\tLoss: 0.715832\n",
      "Train Epoch: 38 [5120/6273 (80%)]\tLoss: 0.604560\n",
      "\n",
      "Test set: Accuracy: 445/837 (53%)\n",
      "\n",
      "Train Epoch: 39 [0/6273 (0%)]\tLoss: 0.863840\n",
      "Train Epoch: 39 [2560/6273 (40%)]\tLoss: 0.491793\n",
      "Train Epoch: 39 [5120/6273 (80%)]\tLoss: 0.679764\n",
      "\n",
      "Test set: Accuracy: 422/837 (50%)\n",
      "\n",
      "Train Epoch: 40 [0/6273 (0%)]\tLoss: 0.771733\n",
      "Train Epoch: 40 [2560/6273 (40%)]\tLoss: 0.805581\n",
      "Train Epoch: 40 [5120/6273 (80%)]\tLoss: 0.686772\n",
      "\n",
      "Test set: Accuracy: 470/837 (56%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_interval = 20\n",
    "for epoch in range(1, 41):\n",
    "    if epoch == 31:\n",
    "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
    "    scheduler.step()\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
